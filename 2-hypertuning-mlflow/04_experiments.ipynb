{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook: CNN Architecture Exploration\n",
    "\n",
    "This notebook follows a hypothesis-driven approach to explore CNN architectures with:\n",
    "- Dropout layers\n",
    "- Normalization techniques\n",
    "- Various architectural patterns\n",
    "- Hyperparameter interactions\n",
    "\n",
    "We'll pause at each section to form hypotheses before running experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from loguru import logger\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Dataset and training utilities\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import metrics, Trainer, TrainerSettings, ReportTypes\n",
    "from mltrainer.imagemodels import CNNConfig, CNNblocks\n",
    "from torchinfo import summary\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 01:51:58 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/22 01:51:58 INFO mlflow.store.db.utils: Updating database tables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "ename": "CommandError",
     "evalue": "Can't locate revision identified by '71994744cf8e'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResolutionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/base.py:254\u001b[39m, in \u001b[36mScriptDirectory._catch_revision_errors\u001b[39m\u001b[34m(self, ancestor, multiple_heads, start, end, resolution)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m revision.RangeNotAncestorError \u001b[38;5;28;01mas\u001b[39;00m rna:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/base.py:462\u001b[39m, in \u001b[36mScriptDirectory._upgrade_revs\u001b[39m\u001b[34m(self, destination, current_rev)\u001b[39m\n\u001b[32m    455\u001b[39m revs = \u001b[38;5;28mself\u001b[39m.iterate_revisions(\n\u001b[32m    456\u001b[39m     destination, current_rev, implicit_base=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m )\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    459\u001b[39m     migration.MigrationStep.upgrade_from_script(\n\u001b[32m    460\u001b[39m         \u001b[38;5;28mself\u001b[39m.revision_map, script\n\u001b[32m    461\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m script \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(revs))\n\u001b[32m    463\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/revision.py:814\u001b[39m, in \u001b[36mRevisionMap.iterate_revisions\u001b[39m\u001b[34m(self, upper, lower, implicit_base, inclusive, assert_relative_length, select_for_downgrade)\u001b[39m\n\u001b[32m    812\u001b[39m     fn = \u001b[38;5;28mself\u001b[39m._collect_upgrade_revisions\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m revisions, heads = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclusive\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclusive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimplicit_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimplicit_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_relative_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43massert_relative_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._topological_sort(revisions, heads):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/revision.py:1475\u001b[39m, in \u001b[36mRevisionMap._collect_upgrade_revisions\u001b[39m\u001b[34m(self, upper, lower, inclusive, implicit_base, assert_relative_length)\u001b[39m\n\u001b[32m   1469\u001b[39m required_node_set = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m   1470\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_ancestor_nodes(\n\u001b[32m   1471\u001b[39m         targets, check=\u001b[38;5;28;01mTrue\u001b[39;00m, include_dependencies=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ).union(targets)\n\u001b[32m-> \u001b[39m\u001b[32m1475\u001b[39m current_revisions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_revisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m implicit_base \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1477\u001b[39m     rev \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m required_node_set\n\u001b[32m   1478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rev \u001b[38;5;129;01min\u001b[39;00m current_revisions\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1480\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/revision.py:542\u001b[39m, in \u001b[36mRevisionMap.get_revisions\u001b[39m\u001b[34m(self, id_)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(id_, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_revisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_elem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mid_elem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m, ())\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/revision.py:542\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(id_, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_revisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_elem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m id_elem \u001b[38;5;129;01min\u001b[39;00m id_], ())\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/revision.py:565\u001b[39m, in \u001b[36mRevisionMap.get_revisions\u001b[39m\u001b[34m(self, id_)\u001b[39m\n\u001b[32m    564\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mself\u001b[39m._revision_for_ident(rev_id, branch_label)\n\u001b[32m    567\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rev_id \u001b[38;5;129;01min\u001b[39;00m resolved_id\n\u001b[32m    568\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/revision.py:566\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    564\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_revision_for_ident\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrev_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rev_id \u001b[38;5;129;01min\u001b[39;00m resolved_id\n\u001b[32m    568\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/revision.py:637\u001b[39m, in \u001b[36mRevisionMap._revision_for_ident\u001b[39m\u001b[34m(self, resolved_id, check_branch)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m revs:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResolutionError(\n\u001b[32m    638\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo such revision or branch \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    639\u001b[39m         % (\n\u001b[32m    640\u001b[39m             resolved_id,\n\u001b[32m    641\u001b[39m             (\n\u001b[32m    642\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m; please ensure at least four characters are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    643\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresent for partial revision identifier matches\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    644\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(resolved_id) < \u001b[32m4\u001b[39m\n\u001b[32m    645\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    646\u001b[39m             ),\n\u001b[32m    647\u001b[39m         ),\n\u001b[32m    648\u001b[39m         resolved_id,\n\u001b[32m    649\u001b[39m     )\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(revs) > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mResolutionError\u001b[39m: No such revision or branch '71994744cf8e'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mCommandError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m experiment_path = \u001b[33m\"\u001b[39m\u001b[33mcnn_architecture_exploration\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m mlflow.set_tracking_uri(\u001b[33m\"\u001b[39m\u001b[33msqlite:///mlflow.db\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMLflow experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMLflow UI available at: http://127.0.0.1:5001\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/fluent.py:153\u001b[39m, in \u001b[36mset_experiment\u001b[39m\u001b[34m(experiment_name, experiment_id)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (experiment_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    146\u001b[39m     experiment_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    147\u001b[39m ):\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    149\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[33mMust specify exactly one of: `experiment_id` or `experiment_name`.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m         error_code=INVALID_PARAMETER_VALUE,\n\u001b[32m    151\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m client = \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/client.py:159\u001b[39m, in \u001b[36mMlflowClient.__init__\u001b[39m\u001b[34m(self, tracking_uri, registry_uri)\u001b[39m\n\u001b[32m    157\u001b[39m final_tracking_uri = utils._resolve_tracking_uri(tracking_uri)\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m._registry_uri = registry_utils._resolve_registry_uri(registry_uri, tracking_uri)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28mself\u001b[39m._tracking_client = \u001b[43mTrackingServiceClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_tracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py:84\u001b[39m, in \u001b[36mTrackingServiceClient.__init__\u001b[39m\u001b[34m(self, tracking_uri)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.tracking_uri = tracking_uri\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# NB: Fetch the tracking store (`self.store`) upon client initialization to ensure that\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# the tracking URI is valid and the store can be properly resolved. We define `store` as a\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# property method to ensure that the client is serializable, even if the store is not\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# self.store\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py:88\u001b[39m, in \u001b[36mTrackingServiceClient.store\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstore\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_store\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:210\u001b[39m, in \u001b[36m_get_store\u001b[39m\u001b[34m(store_uri, artifact_uri)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_store\u001b[39m(store_uri=\u001b[38;5;28;01mNone\u001b[39;00m, artifact_uri=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tracking_store_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/registry.py:45\u001b[39m, in \u001b[36mTrackingStoreRegistry.get_store\u001b[39m\u001b[34m(self, store_uri, artifact_uri)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtracking\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tracking_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m     44\u001b[39m resolved_store_uri = utils._resolve_tracking_uri(store_uri)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_store_with_resolved_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_store_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/registry.py:56\u001b[39m, in \u001b[36mTrackingStoreRegistry._get_store_with_resolved_uri\u001b[39m\u001b[34m(self, resolved_store_uri, artifact_uri)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _building_store_lock:\n\u001b[32m     55\u001b[39m     builder = \u001b[38;5;28mself\u001b[39m.get_store_builder(resolved_store_uri)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_store_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:142\u001b[39m, in \u001b[36m_get_sqlalchemy_store\u001b[39m\u001b[34m(store_uri, artifact_uri)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m artifact_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    141\u001b[39m     artifact_uri = DEFAULT_LOCAL_FILE_AND_ARTIFACT_PATH\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSqlAlchemyStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/store/tracking/sqlalchemy_store.py:176\u001b[39m, in \u001b[36mSqlAlchemyStore.__init__\u001b[39m\u001b[34m(self, db_uri, default_artifact_root)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# On a completely fresh MLflow installation against an empty database (verify database\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# emptiness by checking that 'experiments' etc aren't in the list of table names), run all\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# DB migrations\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mlflow.store.db.utils._all_tables_exist(\u001b[38;5;28mself\u001b[39m.engine):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_initialize_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m SessionMaker = sqlalchemy.orm.sessionmaker(bind=\u001b[38;5;28mself\u001b[39m.engine)\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m.ManagedSessionMaker = mlflow.store.db.utils._get_managed_session_maker(\n\u001b[32m    179\u001b[39m     SessionMaker, \u001b[38;5;28mself\u001b[39m.db_type\n\u001b[32m    180\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/store/db/utils.py:105\u001b[39m, in \u001b[36m_initialize_tables\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m    103\u001b[39m _logger.info(\u001b[33m\"\u001b[39m\u001b[33mCreating initial MLflow database tables...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m InitialBase.metadata.create_all(engine)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43m_upgrade_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/store/db/utils.py:230\u001b[39m, in \u001b[36m_upgrade_db\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m engine.begin() \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[32m    229\u001b[39m     config.attributes[\u001b[33m\"\u001b[39m\u001b[33mconnection\u001b[39m\u001b[33m\"\u001b[39m] = connection\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43mcommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupgrade\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/command.py:408\u001b[39m, in \u001b[36mupgrade\u001b[39m\u001b[34m(config, revision, sql, tag)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m script._upgrade_revs(revision, rev)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m EnvironmentContext(\n\u001b[32m    400\u001b[39m     config,\n\u001b[32m    401\u001b[39m     script,\n\u001b[32m   (...)\u001b[39m\u001b[32m    406\u001b[39m     tag=tag,\n\u001b[32m    407\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[43mscript\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/base.py:586\u001b[39m, in \u001b[36mScriptDirectory.run_env\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_env\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    578\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run the script environment.\u001b[39;00m\n\u001b[32m    579\u001b[39m \n\u001b[32m    580\u001b[39m \u001b[33;03m    This basically runs the ``env.py`` script present\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    584\u001b[39m \n\u001b[32m    585\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_python_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menv.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/util/pyfiles.py:95\u001b[39m, in \u001b[36mload_python_file\u001b[39m\u001b[34m(dir_, filename)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ext == \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(path):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         module = \u001b[43mload_module_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     97\u001b[39m         pyc_path = pyc_file_from_path(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/util/pyfiles.py:113\u001b[39m, in \u001b[36mload_module_py\u001b[39m\u001b[34m(module_id, path)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m spec\n\u001b[32m    112\u001b[39m module = importlib.util.module_from_spec(spec)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/store/db_migrations/env.py:84\u001b[39m\n\u001b[32m     82\u001b[39m     run_migrations_offline()\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[43mrun_migrations_online\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/mlflow/store/db_migrations/env.py:78\u001b[39m, in \u001b[36mrun_migrations_online\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     73\u001b[39m context.configure(\n\u001b[32m     74\u001b[39m     connection=connection, target_metadata=target_metadata, render_as_batch=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     75\u001b[39m )\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context.begin_transaction():\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_migrations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:8\u001b[39m, in \u001b[36mrun_migrations\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/runtime/environment.py:946\u001b[39m, in \u001b[36mEnvironmentContext.run_migrations\u001b[39m\u001b[34m(self, **kw)\u001b[39m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._migration_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Operations.context(\u001b[38;5;28mself\u001b[39m._migration_context):\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_migrations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/runtime/migration.py:611\u001b[39m, in \u001b[36mMigrationContext.run_migrations\u001b[39m\u001b[34m(self, **kw)\u001b[39m\n\u001b[32m    608\u001b[39m head_maintainer = HeadMaintainer(\u001b[38;5;28mself\u001b[39m, heads)\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._migrations_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_migrations_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    612\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.begin_transaction(_per_migration=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    613\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.as_sql \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m head_maintainer.heads:\n\u001b[32m    614\u001b[39m             \u001b[38;5;66;03m# for offline mode, include a CREATE TABLE from\u001b[39;00m\n\u001b[32m    615\u001b[39m             \u001b[38;5;66;03m# the base\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/command.py:397\u001b[39m, in \u001b[36mupgrade.<locals>.upgrade\u001b[39m\u001b[34m(rev, context)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupgrade\u001b[39m(rev, context):\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscript\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_upgrade_revs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrev\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/base.py:450\u001b[39m, in \u001b[36mScriptDirectory._upgrade_revs\u001b[39m\u001b[34m(self, destination, current_rev)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_upgrade_revs\u001b[39m(\n\u001b[32m    448\u001b[39m     \u001b[38;5;28mself\u001b[39m, destination: \u001b[38;5;28mstr\u001b[39m, current_rev: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m    449\u001b[39m ) -> List[RevisionStep]:\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_catch_revision_errors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mancestor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDestination \u001b[39;49m\u001b[38;5;132;43;01m%(end)s\u001b[39;49;00m\u001b[33;43m is not a valid upgrade \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget from current head(s)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterate_revisions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_rev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimplicit_base\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmigration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMigrationStep\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupgrade_from_script\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrevision_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrevs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.9-macos-aarch64-none/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MADS-MachineLearning-course/.venv/lib/python3.11/site-packages/alembic/script/base.py:286\u001b[39m, in \u001b[36mScriptDirectory._catch_revision_errors\u001b[39m\u001b[34m(self, ancestor, multiple_heads, start, end, resolution)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    283\u001b[39m         resolution = \u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt locate revision identified by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % (\n\u001b[32m    284\u001b[39m             re.argument\n\u001b[32m    285\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m util.CommandError(resolution) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m revision.RevisionError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m util.CommandError(err.args[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mCommandError\u001b[39m: Can't locate revision identified by '71994744cf8e'"
     ]
    }
   ],
   "source": [
    "# MLflow setup - using the exact same pattern as 03_mlflow.py\n",
    "experiment_path = \"cnn_architecture_exploration\"\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\"MLflow experiment: {experiment_path}\")\n",
    "print(\"MLflow UI available at: http://127.0.0.1:5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 01:31:53.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /Users/DINGZEEFS/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2025-09-22 01:31:53.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /Users/DINGZEEFS/.cache/mads_datasets/fashionmnist/fashionmnist.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 1, 28, 28])\n",
      "Label shape: torch.Size([64])\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Load FASHION dataset\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "batchsize = 64\n",
    "preprocessor = BasePreprocessor()\n",
    "\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=batchsize, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "\n",
    "# Get sample batch\n",
    "x, y = next(iter(trainstreamer))\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Label shape: {y.shape}\")\n",
    "print(f\"Number of classes: {y.unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CNN Model\n",
    "\n",
    "Let's start with our baseline model from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated matrix size: 9\n",
      "Caluclated flatten size: 288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNblocks                                [64, 10]                  --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─ConvBlock: 2-1                    [64, 32, 28, 28]          --\n",
       "│    │    └─Sequential: 3-1              [64, 32, 28, 28]          9,568\n",
       "│    └─ConvBlock: 2-2                    [64, 32, 28, 28]          --\n",
       "│    │    └─Sequential: 3-2              [64, 32, 28, 28]          18,496\n",
       "│    └─ReLU: 2-3                         [64, 32, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-4                    [64, 32, 9, 9]            --\n",
       "│    └─ConvBlock: 2-5                    [64, 32, 9, 9]            --\n",
       "│    │    └─Sequential: 3-3              [64, 32, 9, 9]            18,496\n",
       "│    └─ReLU: 2-6                         [64, 32, 9, 9]            --\n",
       "│    └─ConvBlock: 2-7                    [64, 32, 9, 9]            --\n",
       "│    │    └─Sequential: 3-4              [64, 32, 9, 9]            18,496\n",
       "│    └─ReLU: 2-8                         [64, 32, 9, 9]            --\n",
       "│    └─MaxPool2d: 2-9                    [64, 32, 3, 3]            --\n",
       "│    └─ConvBlock: 2-10                   [64, 32, 3, 3]            --\n",
       "│    │    └─Sequential: 3-5              [64, 32, 3, 3]            18,496\n",
       "│    └─ReLU: 2-11                        [64, 32, 3, 3]            --\n",
       "├─Sequential: 1-2                        [64, 10]                  --\n",
       "│    └─Flatten: 2-12                     [64, 288]                 --\n",
       "│    └─Linear: 2-13                      [64, 32]                  9,248\n",
       "│    └─ReLU: 2-14                        [64, 32]                  --\n",
       "│    └─Linear: 2-15                      [64, 10]                  330\n",
       "==========================================================================================\n",
       "Total params: 93,130\n",
       "Trainable params: 93,130\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.61\n",
       "==========================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 57.01\n",
       "Params size (MB): 0.37\n",
       "Estimated Total Size (MB): 57.58\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model configuration\n",
    "baseline_config = CNNConfig(\n",
    "    matrixshape=(28, 28),\n",
    "    batchsize=batchsize,\n",
    "    input_channels=1,\n",
    "    hidden=32,  # number of filters\n",
    "    kernel_size=3,\n",
    "    maxpool=3,\n",
    "    num_layers=4,\n",
    "    num_classes=10,\n",
    ")\n",
    "\n",
    "baseline_model = CNNblocks(baseline_config)\n",
    "summary(baseline_model, input_size=(batchsize, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings for quick experiments\n",
    "def create_trainer_settings(logdir=\"models\", epochs=5, train_steps=100, valid_steps=50):\n",
    "    return TrainerSettings(\n",
    "        epochs=epochs,\n",
    "        metrics=[metrics.Accuracy()],\n",
    "        logdir=Path(logdir).resolve(),\n",
    "        train_steps=train_steps,\n",
    "        valid_steps=valid_steps,\n",
    "        reporttypes=[ReportTypes.MLFLOW, ReportTypes.TOML],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, settings, run_name=\"baseline\", log_params=None):\n",
    "    \"\"\"Helper function to train a model with MLflow tracking\"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log parameters\n",
    "        if log_params:\n",
    "            mlflow.log_params(log_params)\n",
    "        \n",
    "        # Set tags\n",
    "        mlflow.set_tag(\"model_type\", model.__class__.__name__)\n",
    "        mlflow.set_tag(\"experiment_phase\", run_name.split(\"_\")[0])\n",
    "        \n",
    "        # Initialize training components\n",
    "        optimizer = optim.Adam\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        accuracy = metrics.Accuracy()\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.loop()\n",
    "        \n",
    "        # Log final metrics\n",
    "        mlflow.log_metric(\"final_train_loss\", trainer.train_loss)\n",
    "        mlflow.log_metric(\"final_valid_loss\", trainer.test_loss)\n",
    "        \n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training settings for quick experiments - using MLflow reporting like 03_mlflow.py\n",
    "def create_trainer_settings(logdir=\"models\", epochs=5, train_steps=100, valid_steps=50):\n",
    "    return TrainerSettings(\n",
    "        epochs=epochs,\n",
    "        metrics=[metrics.Accuracy()],\n",
    "        logdir=Path(logdir).resolve(),\n",
    "        train_steps=train_steps,\n",
    "        valid_steps=valid_steps,\n",
    "        reporttypes=[ReportTypes.MLFLOW, ReportTypes.TOML],  # This is the key - automatic MLflow logging!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, settings, run_name=\"baseline\", log_params=None):\n",
    "    \"\"\"Helper function to train a model with MLflow tracking - following 03_mlflow.py pattern\"\"\"\n",
    "    # Start MLflow run like in the working example\n",
    "    with mlflow.start_run():\n",
    "        # Set tags\n",
    "        mlflow.set_tag(\"model_type\", model.__class__.__name__)\n",
    "        mlflow.set_tag(\"experiment_phase\", run_name.split(\"_\")[0])\n",
    "        mlflow.set_tag(\"dev\", \"student\")\n",
    "        \n",
    "        # Log parameters\n",
    "        if log_params:\n",
    "            mlflow.log_params(log_params)\n",
    "        \n",
    "        # Initialize training components\n",
    "        optimizer = optim.Adam\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Create trainer - the trainer will automatically log to MLflow due to ReportTypes.MLFLOW\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        # Train - MLflow logging happens automatically\n",
    "        trainer.loop()\n",
    "        \n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 01:32:32.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to /Users/DINGZEEFS/MADS-MachineLearning-course/notebooks/2_convolutions/models/20250922-013232\u001b[0m\n",
      "\u001b[32m2025-09-22 01:32:32.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:02<00:00, 34.82it/s]\n",
      "\u001b[32m2025-09-22 01:32:35.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.7069 test 0.6736 metric ['0.7412']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:04<00:00, 20.61it/s]\n",
      "\u001b[32m2025-09-22 01:32:40.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.6389 test 0.6396 metric ['0.7550']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:02<00:00, 36.26it/s]\n",
      "\u001b[32m2025-09-22 01:32:43.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.6226 test 0.6020 metric ['0.7734']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:11<00:00,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Results:\n",
      "Available trainer attributes:\n",
      "  loss_fn: CrossEntropyLoss()\n",
      "  test_loss: 0.6020043933391571\n",
      "\n",
      "Experiment logged to MLflow. View at: http://127.0.0.1:5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run baseline experiment\n",
    "print(\"Training baseline model...\")\n",
    "baseline_settings = create_trainer_settings(epochs=3)\n",
    "\n",
    "baseline_params = {\n",
    "    \"model\": \"baseline_cnn\",\n",
    "    \"filters\": 32,\n",
    "    \"layers\": 4,\n",
    "    \"kernel_size\": 3,\n",
    "    \"dropout\": 0.0,\n",
    "    \"normalization\": \"none\",\n",
    "    \"batch_size\": batchsize\n",
    "}\n",
    "\n",
    "baseline_trainer = train_model(\n",
    "    baseline_model, \n",
    "    baseline_settings, \n",
    "    run_name=\"baseline_cnn\",\n",
    "    log_params=baseline_params\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "# Let's inspect what attributes the trainer actually has\n",
    "print(\"Available trainer attributes:\")\n",
    "for attr in dir(baseline_trainer):\n",
    "    if not attr.startswith('_') and 'loss' in attr.lower():\n",
    "        print(f\"  {attr}: {getattr(baseline_trainer, attr, 'N/A')}\")\n",
    "\n",
    "# Try to access the correct attributes\n",
    "try:\n",
    "    if hasattr(baseline_trainer, 'train_losses') and baseline_trainer.train_losses:\n",
    "        print(f\"Final Train Loss: {baseline_trainer.train_losses[-1]:.4f}\")\n",
    "    if hasattr(baseline_trainer, 'valid_losses') and baseline_trainer.valid_losses:\n",
    "        print(f\"Final Valid Loss: {baseline_trainer.valid_losses[-1]:.4f}\")\n",
    "    elif hasattr(baseline_trainer, 'test_losses') and baseline_trainer.test_losses:\n",
    "        print(f\"Final Valid Loss: {baseline_trainer.test_losses[-1]:.4f}\")\n",
    "    if hasattr(baseline_trainer, 'train_metrics') and baseline_trainer.train_metrics:\n",
    "        print(f\"Final Accuracy: {baseline_trainer.train_metrics[-1][0]:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing trainer attributes: {e}\")\n",
    "    print(\"Let's check all attributes:\")\n",
    "    print([attr for attr in dir(baseline_trainer) if not attr.startswith('_')])\n",
    "\n",
    "print(f\"\\nExperiment logged to MLflow. View at: http://127.0.0.1:5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline experiment\n",
    "print(\"Training baseline model...\")\n",
    "baseline_settings = create_trainer_settings(epochs=3)\n",
    "\n",
    "baseline_params = {\n",
    "    \"model\": \"baseline_cnn\",\n",
    "    \"filters\": 32,\n",
    "    \"layers\": 4,\n",
    "    \"kernel_size\": 3,\n",
    "    \"dropout\": 0.0,\n",
    "    \"normalization\": \"none\",\n",
    "    \"batch_size\": batchsize\n",
    "}\n",
    "\n",
    "baseline_trainer = train_model(\n",
    "    baseline_model, \n",
    "    baseline_settings, \n",
    "    run_name=\"baseline_cnn\",\n",
    "    log_params=baseline_params\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "# Use the exact same pattern as the working 03_mlflow.py example\n",
    "print(f\"Final Test Loss: {baseline_trainer.test_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nExperiment should now be visible in MLflow UI at: http://127.0.0.1:5001\")\n",
    "print(\"Check the 'cnn_architecture_exploration' experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Dropout Experiments\n",
    "\n",
    "Now let's explore the effect of dropout layers.\n",
    "\n",
    "### 🔬 Dropout Hypothesis\n",
    "\n",
    "**Before we add dropout, form a hypothesis:**\n",
    "\n",
    "Questions to consider:\n",
    "- How will dropout affect the training/validation accuracy gap?\n",
    "- What dropout rate will work best (0.1, 0.3, 0.5)?\n",
    "- Where should dropout be placed for maximum effect?\n",
    "- Will dropout slow down training?\n",
    "\n",
    "**Please write your hypothesis below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 Your Dropout Hypothesis:\n",
    "\n",
    "*(Double-click to edit and write your hypothesis here)*\n",
    "\n",
    "- Effect on overfitting:\n",
    "- Optimal dropout rate:\n",
    "- Best placement:\n",
    "- Training speed impact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout CNN Model Implementation\n",
    "class DropoutCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3, filters=32, input_size=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.filters = filters\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(1, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "        \n",
    "        # Calculate size after convolutions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_size)\n",
    "            conv_output = self.conv_layers(dummy_input)\n",
    "            flattened_size = conv_output.view(1, -1).size(1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test different dropout rates\n",
    "dropout_rates = [0.0, 0.1, 0.3, 0.5]\n",
    "dropout_results = []\n",
    "\n",
    "print(\"Running dropout experiments...\")\n",
    "print(\"This will test dropout rates: \", dropout_rates)\n",
    "print(\"\\nRunning experiments (this may take a few minutes)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Normalization Experiments\n",
    "\n",
    "*(We'll add this section after completing dropout experiments)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for each dropout rate\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing dropout rate: {dropout_rate}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = DropoutCNN(dropout_rate=dropout_rate)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training parameters\n",
    "    dropout_params = {\n",
    "        \"model\": \"dropout_cnn\",\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"filters\": 32,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": batchsize\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    settings = create_trainer_settings(epochs=3)\n",
    "    trainer = train_model(\n",
    "        model, \n",
    "        settings, \n",
    "        run_name=f\"dropout_{dropout_rate}\",\n",
    "        log_params=dropout_params\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"test_loss\": trainer.test_loss,\n",
    "        \"model_size\": sum(p.numel() for p in model.parameters())\n",
    "    }\n",
    "    dropout_results.append(result)\n",
    "    \n",
    "    print(f\"Results: Test Loss = {trainer.test_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DROPOUT EXPERIMENTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for result in dropout_results:\n",
    "    print(f\"Dropout {result['dropout_rate']:.1f}: Test Loss = {result['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "## Part 3: Normalization Experiments\n",
    "\n",
    "### 🔬 Normalization Hypothesis\n",
    "\n",
    "**Before we test different normalization techniques, form a hypothesis:**\n",
    "\n",
    "Questions to consider:\n",
    "- How will BatchNorm affect training speed and stability?\n",
    "- Will LayerNorm or InstanceNorm work better for image data?\n",
    "- How will normalization affect the final accuracy?\n",
    "- Should normalization be applied before or after activation functions?\n",
    "\n",
    "**Please write your normalization hypothesis below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Combined Architecture\n",
    "\n",
    "*(We'll add this section after completing normalization experiments)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📝 Your Normalization Hypothesis:\n",
    "\n",
    "*(Double-click to edit and write your hypothesis here)*\n",
    "\n",
    "- BatchNorm impact on training:\n",
    "- Best normalization technique for CNNs:\n",
    "- Accuracy improvement expected:\n",
    "- Placement strategy (before/after activation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization CNN Models\n",
    "class NormalizationCNN(nn.Module):\n",
    "    def __init__(self, norm_type=\"batch\", filters=32, input_size=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.norm_type = norm_type\n",
    "        self.filters = filters\n",
    "        \n",
    "        # Define normalization layer factory\n",
    "        def get_norm_layer(channels):\n",
    "            if norm_type == \"batch\":\n",
    "                return nn.BatchNorm2d(channels)\n",
    "            elif norm_type == \"instance\":\n",
    "                return nn.InstanceNorm2d(channels)\n",
    "            elif norm_type == \"layer\":\n",
    "                return nn.GroupNorm(1, channels)  # LayerNorm equivalent for 2D\n",
    "            else:  # \"none\"\n",
    "                return nn.Identity()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(1, filters, kernel_size=3, padding=1),\n",
    "            get_norm_layer(filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Second conv block  \n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            get_norm_layer(filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, padding=1),\n",
    "            get_norm_layer(filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # Calculate size after convolutions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_size)\n",
    "            conv_output = self.conv_layers(dummy_input)\n",
    "            flattened_size = conv_output.view(1, -1).size(1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test different normalization techniques\n",
    "norm_types = [\"none\", \"batch\", \"instance\", \"layer\"]\n",
    "norm_results = []\n",
    "\n",
    "print(\"Running normalization experiments...\")\n",
    "print(\"This will test normalization types: \", norm_types)\n",
    "print(\"\\nRunning experiments (this may take a few minutes)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Hyperparameter Search\n",
    "\n",
    "*(We'll add this section after completing architecture experiments)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Analysis & Reflection\n",
    "\n",
    "*(We'll complete this section at the end)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
